{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "def remove_stop_punct(s):\n",
    "        from nltk.corpus import stopwords\n",
    "        stopwords=stopwords.words('english')\n",
    "        doc=nlp(s)\n",
    "        #k=sorted([\".\",\"DT\",\"TO\",\"CC\",\"IN\"])\n",
    "        #st=\" \".join(t.text for t in doc if (t.pos_ not in [\"PUNCT\"] )and(t.text.lower() not in stopwords))\n",
    "        st=\" \".join(t.text for t in doc if (t.pos_ not in [\"PUNCT\"] ))\n",
    "        return st.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected unicode, got str)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-79282e2cb506>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtxt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m374\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mremove_stop_punct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-9dcabb4246a0>\u001b[0m in \u001b[0;36mremove_stop_punct\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mstopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdoc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;31m#k=sorted([\".\",\"DT\",\"TO\",\"CC\",\"IN\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"PUNCT\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;32mand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/judson/anaconda2/lib/python2.7/site-packages/spacy/language.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable)\u001b[0m\n\u001b[0;32m    344\u001b[0m             raise ValueError(Errors.E088.format(length=len(text),\n\u001b[0;32m    345\u001b[0m                                                 max_length=self.max_length))\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/judson/anaconda2/lib/python2.7/site-packages/spacy/language.pyc\u001b[0m in \u001b[0;36mmake_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected unicode, got str)"
     ]
    }
   ],
   "source": [
    "f=open('/home/judson/Desktop/sentenceSeg/POS_IntraSent/decision_POS.txt',\"a\")\n",
    "f1=open('/home/judson/Desktop/sentenceSeg/Labelled/decision.txt',\"r\")\n",
    "txt=f1.readlines()\n",
    "for i in txt[374:]:\n",
    "    s=remove_stop_punct(i)\n",
    "    t=nlp(s)\n",
    "    pos=[i.tag_ for i in t]\n",
    "    f.writelines(s+\"\\t\"+\",\".join(pos)+\"\\t<DECISION>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1=open('/home/judson/Desktop/sentenceSeg/Labelled/arguments.txt',\"r\")\n",
    "txt=f1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "def remove_stop_punct(s):\n",
    "        from nltk.corpus import stopwords\n",
    "        stopwords=stopwords.words('english')\n",
    "        doc=nlp(s)\n",
    "        #k=sorted([\".\",\"DT\",\"TO\",\"CC\",\"IN\"])\n",
    "        #st=\" \".join(t.text for t in doc if (t.pos_ not in [\"PUNCT\"] )and(t.text.lower() not in stopwords))\n",
    "        st=\" \".join(t.text for t in doc if (t.pos_ not in [\"PUNCT\"] ))\n",
    "        return st.strip()\n",
    "#print remove_stop_punct(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_conv(st):\n",
    "    doc=nlp(st)\n",
    "    root=[]\n",
    "    for i in doc:\n",
    "        if i.dep_==\"ROOT\":\n",
    "            root.append(i.i)\n",
    "    d={tok.i:[tok.text,tok.dep_,tok.head.text,tok.tag_,[child.i for child in tok.children]] for tok in doc}\n",
    "    return [d,root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "def dfs(sd, start, end = \"$$\"):   \n",
    "    h=networkx.Graph(sd)\n",
    "    ptr=[]\n",
    "    for pth in networkx.all_simple_paths(h,start,\"$$\"):\n",
    "        ptr.append(pth[:-1])\n",
    "    return ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_depend(dt,root):\n",
    "    #md= {i:j[3] for i,j in dt.items()}\n",
    "    #print dt\n",
    "    sd=dict()\n",
    "    sd[\"$$\"]=[]\n",
    "    for i,j in dt.items():\n",
    "        if j[4]==[]:\n",
    "            sd[i]=[\"$$\"]\n",
    "        else:\n",
    "            sd[i]=[k for k in j[4]]\n",
    "    #print sd\n",
    "    #m=\"\"\n",
    "    #for i,j in dt.items():\n",
    "    #    if j[0]==\"ROOT\":\n",
    "    #        print j[0]\n",
    "    #        m=str(i)\n",
    "    #        break\n",
    "    #print m\n",
    "    #print sd\n",
    "    ml=[]\n",
    "    for i in root:\n",
    "        ml.extend(dfs(sd,i,\"$$\"))\n",
    "    #print ml\n",
    "    mpos=[[dt[j][3] for j in i]for i in ml]\n",
    "    mdep=[[dt[j][1] for j in i]for i in ml]\n",
    "    return mdep,mpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input nested string list output string\n",
    "def To_str(nlst):\n",
    "    st=\"(\"\n",
    "    for i in nlst:\n",
    "        st=st+\"(\"\n",
    "        st=st+\",\".join(i)\n",
    "        st=st+\"),\"\n",
    "    return st[:-1]+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/home/judson/Desktop/sentenceSeg/Dep_IntraSent/Dep_decision',\"a\")\n",
    "f1=open('/home/judson/Desktop/sentenceSeg/Labelled/decision.txt',\"r\")\n",
    "decision_pickle=[]\n",
    "txt=f1.readlines()\n",
    "for i in txt:\n",
    "    s=remove_stop_punct(i)\n",
    "    #print s\n",
    "    #t=nlp(s)\n",
    "    dep,_=parse_depend(*dict_conv(s))\n",
    "    decision_pickle.append(dep)\n",
    "    st=To_str(dep)\n",
    "    #f.writelines(s+\"\\t\"+st+\"\\t<DECISION>\\n\")\n",
    "    #print (s+\"\\t\"+st+\"\\t<ARGUMENTS>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open('/home/judson/Desktop/sentenceSeg/Dep_POS_IntraSent/dep_pos_decision.txt',\"a\")\n",
    "f1=open('/home/judson/Desktop/sentenceSeg/Labelled/arguments.txt',\"r\")\n",
    "txt=f1.readlines()\n",
    "for i in txt:\n",
    "    s=remove_stop_punct(i)\n",
    "    #print s\n",
    "    #t=nlp(s)\n",
    "    dep,pos=parse_depend(*dict_conv(s))\n",
    "    st=To_str(dep)\n",
    "    st1=To_str(pos)\n",
    "    f.writelines(s+\"\\t\"+st+\"\\t\"+st1+\"\\t<DECISION>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find unique nested list\n",
    "#input ip_list->nested list\n",
    "#output uniq_list->nested list\n",
    "def uniq_list(ip_list):\n",
    "    uniq_list=[ip_list[0]]\n",
    "    for i in ip_list:\n",
    "        k=0\n",
    "        for j in uniq_list:\n",
    "            if j==i:\n",
    "                k=0\n",
    "                break\n",
    "            else:\n",
    "                k=1\n",
    "        if k==1:\n",
    "            uniq_list.append(i)\n",
    "    return uniq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f=open(\"decision.pickle\",\"wb\")\n",
    "pickle.dump(uniq_list(decision_pickle),f,protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_parse_depend(dt,root):\n",
    "    #md= {i:j[3] for i,j in dt.items()}\n",
    "    #print dt\n",
    "    sd=dict()\n",
    "    sd[\"$$\"]=[]\n",
    "    for i,j in dt.items():\n",
    "        if j[4]==[]:\n",
    "            sd[i]=[\"$$\"]\n",
    "        else:\n",
    "            sd[i]=[k for k in j[4]]\n",
    "    ml=[]\n",
    "    for i in root:\n",
    "        ml.extend(dfs(sd,i,\"$$\"))\n",
    "    #print ml\n",
    "    mpos=[[dt[j][3] for j in i]for i in ml]\n",
    "    mdep=[[dt[j][1] for j in i]for i in ml]\n",
    "    mname=[[dt[j][0] for j in i]for i in ml]\n",
    "    return mdep,mpos,mname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,d,n=mod_parse_depend(*dict_conv(\"Apple is looking at buying U.K.startup for $1 billion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_file_write(p,d,n):\n",
    "    mn=[[] for _ in range(len(p))]\n",
    "    for i in range(len(p)):\n",
    "        mn[i]=[(p[i][j],n[i][j])for j in range(len(p[i]))]\n",
    "    st=\"[\"\n",
    "    for i in mn:\n",
    "        st=st+\"[\"\n",
    "        for j ,k in i:\n",
    "            st=st+\"(\"+j+\",\"+k+\"),\"\n",
    "        st=st[:-1]+\"],\"\n",
    "    st=st[:-7]+\"]]\"\n",
    "    return st+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"/home/judson/Desktop/sentenceSeg/Labelled1/decision.txt\",\"r\")\n",
    "#f1=open(\"/home/judson/LabelledDepparsed.txt\",\"a\")\n",
    "id_r=f.readlines()\n",
    "id_r=id_r[:4]+id_r[5:6]\n",
    "f1.writelines(\"Decision:\\n\")\n",
    "for i in id_r:\n",
    "    f1.writelines(str(i+\":\"+mod_file_write(*mod_parse_depend(*dict_conv(i)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
